{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Term Analysis on Twitter Data\n",
    "\n",
    "## Tasks: \n",
    "\n",
    "- Reading COVID-19 Twitter data from csv file\n",
    "- Removing stopwords, punctuations, URLs, emojis from tweets\n",
    "- Tokenization\n",
    "- Frequency Distribution\n",
    "- Visualizing most frequent unigrams, bigrams\n",
    "\n",
    "__Dataset:__ https://www.kaggle.com/gpreda/covid19-tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import necessary libraries:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits, punctuation\n",
    "import re\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "to_be_removed = (['its', 'much','dont','th','ve'])\n",
    "strange_punctuations = (['‚Äú','‚Äô','‚Ä¶','‚Äú','‚Äô','‚Äù','‚Äù',',','‚Äò','‚Äî','ü¶†','ü§¥','ü§î','üü©','ü§£'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a function to remove common emojis:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a function for tokenization:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a function for preprocessing:__\n",
    "\n",
    "In this step, since tweets are not always written with a perfect spelling, we have to pay attention to the order of text removal. If we run the steps in the wrong order, we might have undesired results.  First, we convert the text to lowercase, then we iterate through the tweets one by one and keep the original text in a variable called __original_t__ for debugging purposes. We remove URLs and emojis in the beginning. We use string library for digits and punctuations and gensim library to remove English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text,to_be_removed):\n",
    "    t_replaced = []\n",
    "    text = text.str.lower()\n",
    "    tmp = \"\"\n",
    "    for i in range(0,len(text)):\n",
    "        # for debugging purposes I create original_t to compare the original text and the preprocessed \n",
    "        # text and check what is omitted at each step\n",
    "        original_t = text[i]\n",
    "        # Remove URLs\n",
    "        tmp = re.sub(r\"http\\S+\",\"\",tmp)\n",
    "        tmp = remove_emoji(tmp)\n",
    "        # Remove punctuations and numbers, I noticed that if I use str.maketrans('', '', punctuation)\n",
    "        # since some users didn't leave a whitespace after the punctuation, sometimes two words get\n",
    "        # together and we miss them in our word count,         \n",
    "        tmp = tmp.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "#       tmp = tmp.translate(str.maketrans('', '', punctuation))\n",
    "        tmp = tmp.translate(str.maketrans('','', digits))\n",
    "        tmp = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', tmp) \n",
    "        tmp = remove_stopwords(tmp)\n",
    "        last_check = [word for word in tmp.split() if word not in to_be_removed]\n",
    "        tmp = ' '.join(last_check)\n",
    "        for j in range(0,len(strange_punctuations)):\n",
    "              tmp = re.sub(strange_punctuations[j],\"\",tmp)\n",
    "        # Add the preprocessed final form of text to list\n",
    "        t_replaced.append(tmp)\n",
    "    return t_replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a function to get bigrams and trigrams in a dataframe sorted by frequency:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_bitri_grams(corpus, n=None):\n",
    "    v = CountVectorizer(ngram_range=(2,3))\n",
    "    X = v.fit_transform(corpus)\n",
    "    bi_tri_df = pd.DataFrame(X.toarray(), columns = v.get_feature_names())\n",
    "    bi_tri_summed = bi_tri_df.sum()\n",
    "    top_n = pd.DataFrame({'ngram': bi_tri_summed.index, 'count':  bi_tri_summed.values})\n",
    "    top_n = top_n.sort_values(by='count', ascending=False).head(n)\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now that we have our functions ready, we can start reading our CSV into a dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/covid19_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Filter out the unnecessary fields in the data and start preprocessing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['user_location', 'text', 'hashtags']]\n",
    "\n",
    "df2.text = preprocess(df2.text, to_be_removed)\n",
    "\n",
    "# Tokenize each row\n",
    "text_tokenized = df2.text.apply(tokenizer)\n",
    "\n",
    "# Create a big list of words\n",
    "long_text = [' '.join(row) for row in text_tokenized]\n",
    "\n",
    "df2.text = long_text\n",
    "long_string = ' '.join(long_text)\n",
    "\n",
    "# Get the frequency distribution\n",
    "fd = nltk.FreqDist([word for word in long_string.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the unigrams we convert FreqDist dictionary to Pandas dataframe, define the column names and sort in descending order. Then we use the __bar plot__ to visualize the most frequent unigrams in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting FreqDist dictionary to Pandas dataframe\n",
    "df_fdist = pd.DataFrame.from_dict(fd, orient='index')\n",
    "df_fdist.columns = ['Count']\n",
    "df_fdist.index.name = 'Unigrams'\n",
    "\n",
    "# Sorting the data frame in descending order\n",
    "df_fdist = df_fdist.sort_values(['Count'], ascending = False)\n",
    "\n",
    "# Visualize the bar plot\n",
    "df_fdist['Count'].head(20).plot(kind='bar')\n",
    "\n",
    "# Plot without the word 'covid'\n",
    "df_fdist['Count'][1:].head(20).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bigrams and trigrams, __to avoid running out of memory__, we divide the dataset into __smaller dataframes__. Then we count the __top 20 bigrams or trigrams__ in each mini batch and we combine the results in one big data frame. After combining them, we group by ngram column and sum the count of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = pd.DataFrame(data = None)\n",
    "# Divide in smaller dataframes to save memory\n",
    "for i in range(0,round(len(df2.text)/10000)):\n",
    "    if(i!=round(len(df2['text'])/10000)-2):\n",
    "        top_10_bi_tri = top_n_bitri_grams(df2['text'][i*10000:((i+1)*10000)+1],20)\n",
    "    else:\n",
    "        top_10_bi_tri = top_n_bitri_grams(df2['text'][i*10000:len(df2['text'])+1],20)\n",
    "    # Gather all top 20 bigrams and trigrams \n",
    "    top_n = top_n.append(top_10_bi_tri)\n",
    "\n",
    "# create a copy of top_n to avoid running the code again if any problems are encountered    \n",
    "copy_counts = top_n\n",
    "\n",
    "# Group by ngram, sum their counts and sort by counts in descending order, show top 20\n",
    "group_by_ngram = copy_counts.groupby('ngram')['count'].sum()\n",
    "group_by_ngram = group_by_ngram.sort_values(ascending = False)\n",
    "group_by_ngram = group_by_ngram[:20]\n",
    "\n",
    "# Plot most frequent bigrams(since there was no trigram in the top 20 list)\n",
    "plt.figure(figsize=(20,8))\n",
    "fig = group_by_ngram.plot(kind='bar')\n",
    "fig.set_title('Most Frequent Bigrams')\n",
    "fig.set_xlabel('bigrams')\n",
    "fig.set_ylabel('count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
